---
title: The Hidden Variance Reduction in Diffusion Loss
date: 2025-12-06
topics: [diffusion, variance-reduction]
---

The variational perspective formulates diffusion models as latent variable models (LVMs) trained by maximizing the evidence lower bound (ELBO). However, standard derivations of the diffusion ELBO often rely on lengthy algebra manipulations to arrive at the final objective function. Why do we go through the trouble of transforming a simple expectation into a complex sum of KL divergences? This post uncovers the statistical motivation behind this derivation: **variance reduction**.

## A Quick Refresher on Latent Variable Models

In a latent variable model (LVM), we assume the observed data $x$ is generated by an unobserved latent factor $z$. The process is defined by a prior $p(z)$ and a likelihood $p(x|z)$. To train a model $p_\theta(x|z)$ to fit the data, we aim to maximize the marginal log-likelihood:

$$
\log p_\theta(x) = \log \int p_\theta(x|z)p(z)dz = \log \mathbb{E}_{z \sim p(z)}[p_\theta(x|z)]
$$

**The Problem:** Computing this expectation directly is intractable. If we try to estimate it via Monte Carlo sampling ($z \sim p(z)$), the variance is explodingly high because for high-dimensional data, $p_\theta(x|z)$ is effectively zero for almost all $z$ sampled from the prior.

### The Evidence Lower Bound (ELBO)

To solve this variance issue, we use **importance sampling**: introduce an approximate posterior $q(z|x)$, referred to as the proposal distribution, that focuses sampling on regions where the likelihood is non-negligible. This yields the tractable evidence lower bound (ELBO):

$$
\begin{align}
\log p_{\theta}(x) &= \log \int \frac{p_{\theta}(x, z)}{q(z \mid x)} \cdot q(z \mid x) dz \\
&= \log \mathbb{E}_{z \sim q(z \mid x)}\left[ \frac{p_{\theta}(x, z)}{q(z \mid x)} \right] \\
&\geq \mathbb{E}_{z \sim q(z \mid x)}\left[ \log \frac{p_{\theta}(x, z)}{q(z \mid x)} \right] && \text{(Jensen's Inequality)}\\
&\coloneqq \mathcal{L}(\theta; x)
\end{align}
$$

Maximizing $\mathcal{L}(\theta; x)$ tightens the lower bound on the log-likelihood $\log p_\theta(x)$. The practical benefit is that by proposing over likely latents via $q(z|x)$, the variance of the estimator is significantly lower than naive sampling, making the Monte Carlo estimate tractable.

## Diffusion Models as Latent Variable Models

Diffusion models are LVMs where the observed data is $x = x_0$ and the latent is $z = x_{1:T}$. The data distribution $q(x_{0:T})$ is defined by a fixed **forward process** $q(x_{t}\mid x_{t-1})$ that progressively adds noise, forming a Markov chain factorized as:

$$
q(x_{0:T}) = q(x_{0})\prod_{t=1}^T q(x_{t}\mid x_{t-1})
$$

By the end of this process, $x_{T}$ is almost pure noise (typically standard Gaussian). To generate samples, we need to run this process in reverse:

$$
\begin{align}
q(x_{0:T}) &= q(x_{T})\prod_{t=T}^1 q(x_{t-1} \mid x_{t:T})  \\
&= q(x_{T})\prod_{t=1}^T q(x_{t-1} \mid x_{t}) && \text{(reverse of Markov is Markov)}
\end{align}
$$

The **true prior** $q(x_{T})$ and the **true reverse process** $q(x_{t-1}\mid x_{t})$ are intractable. Instead, we fix some **approximate prior** $p(x_{T})\approx q(x_{T})$ and learn an **approximate reverse process** $p_{\theta}(x_{t-1}\mid x_{t})\approx q(x_{t-1}\mid x_{t})$. This lets us approximate $q(x_{0:T})$ with $p_{\theta}(x_{0:T})$ defined as:

$$
p_{\theta}(x_{0:T})\coloneqq  p(x_{T})\prod_{t=1}^T p_{\theta}(x_{t-1} \mid x_{t}) \approx q(x_{0:T})
$$

## The Diffusion Loss

Substituting the forward and backward processes into the negative ELBO yields the diffusion loss $L(\theta;x_{0})$. Standard derivations ([Lilian Weng](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/), [Ho et al. 2020](https://arxiv.org/abs/2006.11239), [Sohl-Dickstein et al. 2015](https://arxiv.org/abs/1503.03585)) rewrite this loss as a sum of KL divergences:

$$
\begin{align}
L(\theta; x_{0}) &= \mathbb{E}_{q(x_{1:T} \mid x_{0})}\left[ \log \frac{q(x_{1:T} \mid x_{0})}{p_{\theta}(x_{0:T})} \right] \\
&= \mathbb{E}_{q}\left[ \log \frac{\prod_{t=1}^T q(x_{t} \mid x_{t-1})}{p(x_{T})\prod_{t=1}^T p_{\theta}(x_{t-1}\mid x_{t})} \right] \\
&= \mathbb{E}_{q}\left[ -\log p(x_{T}) + \sum_{t=2}^T \log \frac{q(x_{t} \mid x_{t-1})}{p_{\theta}(x_{t-1} \mid x_{t})} + \log\frac{q(x_{1} \mid x_{0})}{p_{\theta}(x_{0} \mid x_{1})} \right] \\
&\stackrel{\text{(i)}}{=} \mathbb{E}_{q}\left[ -\log p(x_{T}) + \sum_{t=2}^T \log \left( \frac{q(x_{t-1} \mid x_{t}, x_{0})}{p_{\theta}(x_{t-1} \mid x_{t})}\cdot \frac{q(x_{t} \mid x_{0})}{q(x_{t-1}\mid x_{0})} \right) + \log \frac{q(x_{1} \mid x_{0})}{p_{\theta}(x_{0} \mid x_{1})} \right] \\
&\stackrel{\text{(ii)}}{=} \mathbb{E}_{q}\left[ -\log p(x_{T}) + \sum_{t=2}^T \log \frac{q(x_{t-1} \mid x_{t}, x_{0})}{p_{\theta}(x_{t-1} \mid x_{t})} +\log\frac{q(x_{T} \mid x_{0})}{q(x_{1} \mid x_{0})} + \log \frac{q(x_{1} \mid x_{0})}{p_{\theta}(x_{0} \mid x_{1})} \right] \\
&= \mathbb{E}_{q}\left[ \log\frac{q(x_{T} \mid x_{0})}{p(x_{T})} + \sum_{t=2}^T \log \frac{q(x_{t-1} \mid x_{t}, x_{0})}{p_{\theta}(x_{t-1} \mid x_{t})} - \log p_{\theta}(x_{0} \mid x_{1}) \right] \\
&\stackrel{\text{(iii)}}{=} \mathbb{E}_{q(x_T \mid x_0)}\!\left[ \log \frac{q(x_T \mid x_0)}{p(x_{T})} \right] + \sum_{t=2}^{T} \mathbb{E}_{q(x_t \mid x_0)} \, \mathbb{E}_{q(x_{t-1} \mid x_t, x_0)}\!\left[ \log \frac{q(x_{t-1} \mid x_t, x_0)}{p_{\theta}(x_{t-1} \mid x_t)} \right] + \mathbb{E}_{q(x_1 \mid x_0)}\!\left[ - \log p_{\theta}(x_0 \mid x_1) \right] \\
&= \underbrace{D_{\text{KL}}(q(x_T \mid x_0) \parallel p(x_T))}_{L_T} + \sum_{t=2}^{T} \underbrace{\mathbb{E}_{q(x_t \mid x_0)}\left[D_{\text{KL}}(q(x_{t-1} \mid x_t, x_0) \parallel p_{\theta}(x_{t-1} \mid x_t))\right]}_{L_{t-1}} + \underbrace{\mathbb{E}_{q(x_1 \mid x_0)}\left[- \log p_{\theta}(x_0 \mid x_1)\right]}_{L_0}
\end{align}
$$

**Key steps:**

- **Eq (i)** is a clever application of Bayes' rule:
  $$
  \begin{align}
  q(x_{t} \mid x_{t-1}) &= q(x_{t} \mid x_{0}, x_{t-1}) && \text{(Markov property)} \\
  &= \frac{q(x_{t}, x_{0}, x_{t-1})}{q(x_{0}, x_{t-1})} \\
  &= \frac{q(x_{t}, x_{t-1} \mid x_{0})}{q(x_{t-1} \mid x_{0})} \\
  &= \frac{q(x_{t} \mid x_{0})q(x_{t-1} \mid x_{t}, x_{0})}{q(x_{t-1} \mid x_{0})} && \text{(chain rule)}
  \end{align}
  $$
- **Eq (ii)** considers the telescoping sum $\sum_{t=2}^T \log \frac{q(x_{t} \mid x_{0})}{q(x_{t-1} \mid x_{0})} = \log\frac{q(x_{T} \mid x_{0})}{q(x_{1} \mid x_{0})}$.
- **Eq (iii)** uses law of total expectation. For example, the first expectation can be decomposed as:
  $$
  \mathbb{E}_{q(x_{1:T} \mid x_{0})}\left[ \log\frac{q(x_{T} \mid x_{0})}{p(x_{T})} \right] = \mathbb{E}_{q(x_{T} \mid x_{0})}\left[ \mathbb{E}_{q(x_{1:T-1} \mid x_{0}, x_{T})}\left[ \log\frac{q(x_{T} \mid x_{0})}{p(x_{T})} \right] \right]
  = \mathbb{E}_{q(x_{T} \mid x_{0})}\left[ \log\frac{q(x_{T} \mid x_{0})}{p(x_{T})} \right]
  $$

### But Why?

Often by design, these are KL divergences between Gaussian distributions, for which closed-form solutions exist. But in line 2 of the loss derivation, every term can already be computed. Why go through this algebra to express the objective as a sum of KL divergences?

Surprisingly, most popular treatments of diffusion models present this derivation without explaining its purpose. The only place I have seen it mentioned at all is a single sentence in the DDPM paper ([Ho et al. 2020](https://arxiv.org/abs/2006.11239)). This post aims to fill that gap.

## The Hidden Variance Reduction

The derivation is motivated by a technique widely known in Monte Carlo literature as **Rao-Blackwellization**. By analytically computing the KL divergence, we reduce the variance of the loss estimator compared to a naive Monte Carlo approximation.

_(Note: The classical Rao-Blackwell theorem concerns sufficient statistics, but in Monte Carlo the term "Rao-Blackwellization" is used more broadly for replacing sampling with analytic conditional expectations.)_

### Variance Reduction by Conditioning

**Theorem (Rao-Blackwellization for Monte Carlo).** Let $X, Y$ be random variables and suppose we want to estimate $\mu = \mathbb{E}[f(X, Y)]$. Define $g(X) \coloneqq \mathbb{E}[f(X, Y) \mid X]$. Consider two estimators using $N$ i.i.d. samples:

1. **Monte Carlo:** $\hat{\mu}_{MC} = \frac{1}{N} \sum_{i=1}^N f(x_i, y_i)$ where $(x_i, y_i) \sim p(x, y)$.
2. **Rao-Blackwellized:** $\hat{\mu}_{RB} = \frac{1}{N} \sum_{i=1}^N g(x_i)$ where $x_i \sim p(x)$.

Then:

1. Both estimators are unbiased: $\mathbb{E}[\hat{\mu}_{MC}] = \mathbb{E}[\hat{\mu}_{RB}] = \mu$.
2. The Rao-Blackwellized estimator has lower variance: $\operatorname{Var}(\hat{\mu}_{RB}) \leq \operatorname{Var}(\hat{\mu}_{MC})$.
3. Equality holds if and only if $f$ does not depend on $Y$ (almost surely).

**Proof.** _(Unbiasedness)_ By linearity of expectation:

$$
\mathbb{E}[\hat{\mu}_{MC}] = \frac{1}{N} \sum_{i=1}^N \mathbb{E}[f(x_i, y_i)] = \mu
$$

For the second estimator, the law of iterated expectations gives:

$$
\mathbb{E}[\hat{\mu}_{RB}] = \frac{1}{N} \sum_{i=1}^N \mathbb{E}[g(x_i)] = \frac{1}{N} \sum_{i=1}^N \mathbb{E}[\mathbb{E}[f(X, Y) \mid X]] = \mu
$$

_(Variance Comparison)_ Since the samples are i.i.d., we have $\operatorname{Var}(\hat{\mu}_{MC}) = \frac{1}{N}\operatorname{Var}(f(X, Y))$ and $\operatorname{Var}(\hat{\mu}_{RB}) = \frac{1}{N}\operatorname{Var}(g(X))$. It suffices to show $\operatorname{Var}(g(X)) \leq \operatorname{Var}(f(X, Y))$.

By the law of total variance:

$$
\begin{align}
\operatorname{Var}(f(X, Y)) &= \mathbb{E}\left[\operatorname{Var}(f(X, Y) \mid X)\right] + \operatorname{Var}\left(\mathbb{E}[f(X, Y) \mid X]\right) \\
&= \underbrace{\mathbb{E}\left[\operatorname{Var}(f(X, Y) \mid X)\right]}_{\geq 0} + \operatorname{Var}(g(X)) \\
&\geq \operatorname{Var}(g(X))
\end{align}
$$

Dividing both sides by $N$, we get $\operatorname{Var}(\hat{\mu}_{MC}) \geq \operatorname{Var}(\hat{\mu}_{RB})$. Equality holds if and only if $\operatorname{Var}(f(X, Y) \mid X) = 0$ (almost surely), which occurs precisely when $f$ does not depend on $Y$. $\square$

### Application to Diffusion Models

Let's apply this theorem to the diffusion loss, using $L_{t-1}$ as an example. We wish to estimate:

$$
L_{t-1} = \mathbb{E}_{q(x_{t-1}, x_{t} \mid  x_{0})}\left[ \log \frac{q(x_{t-1} \mid x_t, x_0)}{p_{\theta}(x_{t-1} \mid x_t)} \right]
$$

Setting $X = x_t$, $Y = x_{t-1}$, and $f(X, Y) = \log \frac{q(Y \mid X, x_0)}{p_{\theta}(Y \mid X)}$, the theorem gives us two estimators using $N$ i.i.d. samples:

**Naive Monte Carlo:** Sample $(x_{t-1}^{(i)}, x_t^{(i)}) \sim q(x_{t-1}, x_t \mid x_0)$ for $i = 1, \ldots, N$ and compute:

$$
\hat{L}_{t-1}^{\text{MC}} = \frac{1}{N}\sum_{i=1}^{N} \log \frac{q(x_{t-1}^{(i)} \mid x_t^{(i)}, x_0)}{p_{\theta}(x_{t-1}^{(i)} \mid x_t^{(i)})}
$$

**Rao-Blackwellized:** Sample $x_{t}^{(i)}\sim q(x_{t}\mid x_{0})$ for $i = 1, \ldots, N$ and analytically compute the conditional expectation:

$$
\hat{L}_{t-1}^{\text{RB}} = \frac{1}{N}\sum_{i=1}^{N} \mathbb{E}_{q(x_{t-1} \mid x_t^{(i)}, x_0)}\left[ \log \frac{q(x_{t-1} \mid x_t^{(i)}, x_0)}{p_{\theta}(x_{t-1} \mid x_t^{(i)})} \right] = \frac{1}{N}\sum_{i=1}^{N} D_{\text{KL}}(q(x_{t-1} \mid x_t^{(i)}, x_0) \parallel p_{\theta}(x_{t-1} \mid x_t^{(i)}))
$$

The theorem tells us both estimators are unbiased, but the Rao-Blackwellized version has strictly lower variance (since $f$ depends on $x_{t-1}$).

The Rao-Blackwellized version is precisely the KL divergence formulation derived above! By reducing the variance of our loss estimator, we obtain more reliable gradient estimates during training. Lower variance gradients mean that each training step provides a more consistent signal about which direction to update the parameters, leading to more stable optimization and potentially faster convergence.

## Experiments

To empirically validate this variance reduction benefit, we trained diffusion models on the "Two Moons" dataset and compared two loss formulations:

- **Monte Carlo (MC):** $\hat{L}_{t-1}^{\text{MC}} = \frac{1}{N}\sum_{i=1}^{N} \log \frac{q(x_{t-1}^{(i)} \mid x_t^{(i)}, x_0)}{p_{\theta}(x_{t-1}^{(i)} \mid x_t^{(i)})}$ where $(x_{t-1}^{(i)}, x_t^{(i)}) \sim q(x_{t-1}, x_t \mid x_0)$
- **Rao-Blackwellized (RB):** $\hat{L}_{t-1}^{\text{RB}} = \frac{1}{N}\sum_{i=1}^{N} D_{\text{KL}}(q(x_{t-1} \mid x_t^{(i)}, x_0) \parallel p_{\theta}(x_{t-1} \mid x_t^{(i)}))$ where $x_t^{(i)} \sim q(x_t \mid x_0)$

The first experiment examines training dynamics and generation quality, confirming that while both losses can train diffusion models, the RB formulation offers practical advantages. The second experiment directly measures the variance reduction across diffusion timesteps throughout training.

### Training Dynamics

Two diffusion models with identical architectures were trained: one using the <span style="color: #E74C3C;">RB</span> loss and another using the <span style="color: #3498DB;">MC</span> loss.

The generated samples at different training steps reveal the difference:

![Sample Evolution](sample_evolution.gif)

The RB model forms the two moons shape much earlier, and the final converged model seems to generate noticeably better samples. The training loss and sample quality curves confirm this observation:

![Training Dynamics](training_dynamics.png)

The RB model's loss decays faster and remains consistently lower throughout training. Measuring sample quality via maximum mean discrepancy (MMD, lower is better), the RB model converges faster and achieves superior sample quality.

### Variance Evolution

To directly verify the variance reduction, we trained a single diffusion model and tracked the empirical variance of both loss estimators across all diffusion timesteps $t \in \{1, \ldots, T\}$ throughout training. At regular intervals during training, we:

1. For each timestep $t$, computed both $\hat{L}_{t-1}^{\text{MC}}$ and $\hat{L}_{t-1}^{\text{RB}}$ using a single sample ($N=1$)
2. Estimated the variance of each estimator by repeating this process multiple times

The results are visualized below:

![Variance Evolution](variance_evolution.gif)

The RB estimator consistently exhibits variance on the order of $10^2$ lower than the MC estimator across all diffusion timesteps $t$. It is surprising that despite this dramatic variance difference, the MC model still produces reasonable results. Notably, the shape of the variance curves remains remarkably consistent throughout training, suggesting that the variance reduction benefit is stable across the optimization trajectory.

## Conclusion

The standard derivation of diffusion loss as a sum of KL divergences is not merely algebraic convenience—it is motivated by Rao-Blackwellization, a fundamental variance reduction technique. The algebraic manipulation allows us to reduce loss estimator variance without introducing bias.

Our experiments confirm this matters in practice: the variance reduction ($\sim 10^2$ lower) of the Rao-Blackwellized loss leads to faster convergence, lower final loss, and better sample quality. Surprisingly, the naive Monte Carlo approach still produces reasonable results.
